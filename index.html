<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/png" href="favicon.png" />
    <title>Text Generation with TED - Trainable Exponential Decay(s)</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            color: #333;
            background-color: #f4f4f4;
        }
        .container {
            width: 85%;
            margin: auto;
            overflow: hidden;
        }
        header {
            background: #35424a;
            color: #ffffff;
            padding-top: 30px;
            min-height: 70px;
            border-bottom: #e8491d 3px solid;
        }
        header a {
            color: #ffffff;
            text-decoration: none;
            text-transform: uppercase;
            font-size: 16px;
        }
        header ul {
            padding: 0;
            margin: 0;
            list-style: none;
            overflow: hidden;
        }
        header li {
            float: left;
            display: inline;
            padding: 0 20px 0 20px;
        }
        header #branding {
            float: left;
        }
        header #branding h1 {
            margin: 0;
        }
        header nav {
            float: right;
            margin-top: 10px;
        }
        header .highlight, header .current a {
            color: #e8491d;
            font-weight: bold;
        }
        header a:hover {
            color: #ffffff;
            font-weight: bold;
        }
        @media (max-width: 768px) {
            header #branding {
                float: none;
                text-align: center;
                margin: 10px 0;
            }
            .container {
                width: 100%;
            }
        }
        @media (max-width: 1280px) {
            header #branding {
                float: none;
                text-align: center;
                margin: 10px 0;
            }
        }
        .article {
            padding: 20px;
            background: #ffffff;
            margin-bottom: 20px;
        }
        .article h2, .article p {
            margin-bottom: 20px;
        }

        .abstract {
            background: #ffffff;
            padding: 20px;
            margin-bottom: 20px;
            border-left: 4px solid #e8491d;
        }

        .abstract h2 {
            margin-bottom: 15px;
        }

        .abstract p {
            text-align: justify;
            line-height: 1.6;
        }

        button {
            font-size: large;
        }

        #output-wrapper {
            border-style: dashed;
            border-width: 1px;
            min-height: 150px;
            max-height: 150px;
            overflow-y: scroll;
            padding: 0.5rem;
        }

        #output {
            white-space: pre-wrap;
            display: flex;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            border: 1px solid black;
        }
        th, td {
            border: 1px solid black;
            padding: 8px;
            text-align: left;
        }
        #vocabLifetimeList {
            width: 100px;
            height: 200px;
            overflow-y: auto;
            border: 1px solid #ccc;
            padding: 10px;
            font-family: monospace;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        figure {
            text-align: center;
        }
        figcaption {
            text-align: center;
        }
        .math-container {
            background-color: #f4f4f4;
            border-left: 5px solid #333;
            margin: 20px 0;
            padding: 10px;
        }
        math {
            font-size: 1.1em;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div id="branding">
                <h1><span class="highlight">Text Generation with TED</span> - Trainable Exponential Decay(s)</h1>
            </div>
        </div>
    </header>
    <div class="container">
        <section class="abstract">
            <h2>Abstract</h2>
            <p>
                In contrast to the comprehensive comparisons made by Transformers or the sequential dependency in RNNs,
                Trainable Exponential Decay(s) (TED) introduces a distinct approach that does not directly depend on past tokens. 
                The core mechanism of TED considers each token only once, in isolation, to determine its decay rate, λ (lambda).
                Past tokens are then subjected to exponential decay, and their influence is summed and added to the current token to create rich representations.
                TED benefits from parallel hardware architecture during training, as the only dependency on past tokens is through addition, 
                which can be performed at any time, in any order, within the context of a single layer.
                
                After training the model, one can observe that the influence of past tokens naturally decays over time, allowing for some tokens to become irrelevant and thus droppable from the context.
                Moreover, the well-established mathematical framework surrounding exponential decay enables precise prediction of the moment a particular token will become 'dead'.
                This observation opens up numerous optimization opportunities during inference.
                The simplest strategy involves maintaining the K most relevant tokens at any given time, ensuring constant memory requirements.
                Alternatively, a dynamic approach might involve dropping tokens whose relevance scores fall below a certain threshold.
                The relevance score is calculated by multiplying a token's vector magnitude ‖v‖ by its current decay factor at time 't'.
                
                The order of the past tokens that are kept in the context does not matter, thanks to the nature of exponential decay.
                This eliminates the need for the positional encoding mechanism, which is seen as problematic in Transformers.
                The architecture of TED follows that of a Transformer, with the difference that the attention module is replaced by an exponential decay module.
                TED can be implemented in PyTorch without special CUDA kernels.
                The code is available at: <a href="https://github.com/blpj/ted">https://github.com/blpj/ted</a>
            </p>
            <figure>
                <img src="assets/cat.png">
                <figcaption><small>Tokens decay over time and their influence is summed creating more useful representations.</small></figcaption>
            </figure>
        </section>
    </div>
    <div class="container">
        <section class="article">
            <h2>Tiny Stories TED Model - Demo</h2>
            <p>
                <ul>
                    <li>Proof of Concept</li>
                    <li>Character-level</li>
                    <li>Small, 4.7 Million parameters</li>
                    <li>Undertrained - trained only on a fraction of TinyStoriesV2 dataset <a href="#tinyStoriesPaper">[1]</a><a href="#tinyStoriesHf">[2]</a></li>
                </ul>
            </p>
            <p>
                This is <i>Proof of Concept</i> and it's intended to show that TED can generate plausible text.
                The model was trained using GTX 1060 for 20000 iterations.
            </p>
            <p>
                <strong>It runs in your browser with pure unoptimized JavaScript code without any parallelization.</strong>
            </p>
            <p>
                Download size: 25 MiB
                <p>
                    <strong>Click <i>Load</i> below and wait for it to finish to try the model.</strong>
                </p>
            </p>
            <p>
                <noscript>
                    Your browser does not support JavaScript.   
                </noscript>
                <button onclick="loadTinyStoriesModel()">Load</button>
                <progress id="progressBar" max="100" value="0"></progress>
            </p>
            <p>
                <p>Initial prompt:</p>
                <textarea id="prompt" rows="5", cols="30">Once upon a time</textarea>
            </p>
            <p>
                <button onclick="start()">Start</button>
                <button onclick="stop()">Stop</button>
                <button onclick="step()">Step</button>
                <button onclick="reset()">Reset</button>
            </p>
            <p>
                <pre id="output-wrapper"><code id="output"></code></pre>
            </p>
            <p>
                <p><b>Sampling config:</b></p>
                <label for="topK">Top-K:</label>
                <input type="number" id="topK" name="topK" min="0" max="100">
                <label for="topP">Top-P:</label>
                <input type="number" id="topP" name="topP" min="0" max="1" step="0.05">
                <p>
                    <label for="temperature">Temperature:</label>
                    <input type="number" id="temperature" name="temperature" min="0.01" step="0.05">
                </p>
                <p>
                    <label for="delay">Delay (milliseconds):</label>
                    <input type="number" id="delay" name="delay" min="0" step="1">
                </p>
                <p><b>Context config (all layers):</b></p>
                <label for="keepTopK">How many tokens to keep in a context (Top-K):</label>
                <p><input type="number" id="keepTopK" name="keepTopK" min="0"></p>
                <label for="relevanceScore">The relevance score below which we drop the tokens:</label>
                <p><input type="number" id="relevanceScore" name="relevanceScore" min="0" step="0.01"></p>
                <hr>
                <p>
                    <p>
                        Below table contains dynamic information about number of tokens in a context per layer.
                        Higher layers contain more useful representations,
                        so the model learns to not forget them so quickly - predicts lower decay rate,
                        thus the number of tokens in higher layers is higher.
                        This effect can be seen if the Top-K for <i>Context config</i>
                        is set to <strong>0</strong> - then Top-K strategy is not applied,
                        and the relevance score threshold is greater than <strong>0</strong> - we allow
                        to drop the tokens whose relevance score is below threshold.
                    </p>
                    <table id="data-table">
                        <thead>
                            <tr>
                                <th>Layer Index</th>
                                <th>Context Size</th>
                                <th>Avg. Context Size</th>
                            </tr>
                        </thead>
                        <tbody id="table-body">
                            <!-- Rows will be added here -->
                        </tbody>
                    </table>
                </p>
                <hr>
                <p>
                    <p>
                        For the first layer - layer 0 - we can pre-calculate the particular character 
                        lifetime ahead of time, because the character embeddings, once learnt, stay constant,
                        thus the lambda (λ) parameter also is constant (per character). The calculation depends
                        on the relevance score threshold set above in the <i>Context config</i>.
                        The "lifetime" represents how many discrete steps into the future
                        we need to take until the token is dropped from the context.
                    </p>
                    <small>
                        <i>
                            <p>Format: [character] : [lifetime]</p>
                        </i>
                    </small>
                    <div id="vocabLifetimeList"></div>
                </p>
            </p>
        </section>
        <section class="article">
            <h2>Architecture</h2>
            <p>
                TED architecture is similar to LLaMA <a href="#llama">[3]</a> in some aspects:
                <ul>
                    <li>Pre-normalization with RMSNorm</li>
                    <li>SwiGLU activation function for FeedForward layers</li>
                    <li>No linear biases</li>
                </ul>
                However, there is no need for positional encodings and the attention layer is replaced with exponential decay module.
            </p>
            <figure>
                <img src="assets/architecture.png">
                <figcaption><small>Multiple TED Layers can be stacked together.</small></figcaption>
            </figure>
        </section>
        <section class="article">
            <h2>Exponential Decay Module</h2>
            The Exponential Decay Module is built on the premise that not all tokens carry equal weight as time progresses. 
            By computing a decay factor that exponentially diminishes the influence of older tokens, the model can maintain a compact and relevant context, 
            enabling more efficient processing and potentially enhancing performance on tasks that require a nuanced understanding of temporal dynamics.
            The core idea behind TED is to learn and subsequently predict the appropriate value for the decay constant in the exponential decay equation.
            <p>
                <figure>
                    <img src="assets/exp.png">
                    <figcaption><small>The model learns to predict λ (lambda).</small></figcaption>
                </figure>
            </p>
            <h3>The recipe</h3>
            <p>
                1. Start with input tokens.
                <figure>
                    <img src="assets/input.png">
                    <figcaption><code><small>tokens</small></code></figcaption>
                </figure>
            </p>
            <p>
                2. Use trainable weight matrix to predict the negative value of lambda based on the input.
                <figure>
                    <img src="assets/lambda.png">
                    <figcaption><code><small>negative_lambdas = self.lambda_matrix(tokens).sigmoid().log()</small></code></figcaption>
                </figure>
            </p>
            <p>
                3. Construct time matrix.
                <figure>
                    <img src="assets/time.png">
                    <figcaption><code><small>time = torch.ones(n, n).tril_(-1).cumsum_(-2)</small></code></figcaption>
                </figure>
            </p>
            <p>
                4. Calculate decay factor.
                <figure>
                    <img src="assets/decay_factor.png">
                    <figcaption><code><small>decay_factor = (negative_lambdas.swapaxes(-1, -2) * time).exp().tril()</small></code></figcaption>
                </figure>
            </p>
            <p>
                5. Use trainable matrix to calculate quantity.
                <figure>
                    <img src="assets/quantity.png">
                    <figcaption><code><small>quantities = self.quantity_matrix(tokens)</small></code></figcaption>
                </figure>
            </p>
            <p>
                6. Calculate the influence (output) at every time step.
                <figure>
                    <img src="assets/influence.png">
                    <figcaption><code><small>output = decay_factor @ quantities</small></code></figcaption>
                </figure>
            </p>
            <p>
                7. Use trainable matrices to transform and gate the output as the last step.
                <figure>
                    <img src="assets/final.png">
                    <figcaption><code><small>y = self.output_matrix(F.silu(output)) * self.gate_matrix(tokens)</small></code></figcaption>
                </figure>
            </p>
            <p>
                8. Putting it all together, the transformation performed by the Exponential Decay Module from input tokens X to Y can be expressed as:
                <figure>
                    <img src="assets/full.png">
                </figure>
            </p>
        </section>
        <section class="article">
            <h2>Inference</h2>
            <figure>
                <img src="assets/context.png">
                <figcaption></figcaption>
            </figure>
            <p>
                During inference, the model must efficiently manage the context to maintain performance while conserving computational resources. 
                Past tokens are kept in the context and are subject to an exponential decay, which reduces their relevance over time. 
                This decay simulates the natural process of fading memory, where older information becomes less significant.
                To quantify the fading influence of each token, a relevance score is calculated. 
                This score is derived by multiplying the magnitude of the token's vector representation by an exponential decay factor. 
                                
                <figure>
                    <img src="assets/relevance_score.png">
                    <figcaption><small>The relevance score.</small></figcaption>
                </figure>
                With the relevance score established, one can employ various strategies to manage the context by selectively retaining
                only those tokens that are deemed crucial for the model's current state. 
                These strategies include:
                <ul>
                    <li>
                        <strong>Fixed Memory Strategy</strong>
                        <p>
                            The most straightforward approach is to maintain a fixed number of 
                            K most relevant tokens in the context at all times. This strategy ensures constant memory usage and computational demand. The value of 
                            K is chosen based on a balance between computational efficiency and the model's performance, 
                            ensuring that enough context is preserved for accurate predictions.
                        </p>
                    </li>
                    <li>
                        <strong>Dynamic Threshold Strategy</strong>
                        <p>
                            Alternatively, a dynamic approach can be taken where tokens are kept in the context as long as their relevance score remains above a predefined threshold. 
                            This threshold is pivotal; set too high, it may discard useful context prematurely, while too low may retain superfluous tokens, wasting computational resources.
                            The threshold can be a static value determined through validation, 
                            or it could be adaptively adjusted based on the model's performance metrics or the complexity of the task at hand.
                        </p>
                    </li>
                </ul>
            </p>
        </section>
        <section class="article">
            <h2>References</h2>
            <p id="tinyStoriesPaper">[1]: <a href="https://doi.org/10.48550/arXiv.2305.07759">arXiv.2305.07759</a></p>
            <p id="tinyStoriesHf">[2]: <a href="https://huggingface.co/datasets/noanabeshima/TinyStoriesV2">huggingface.co/datasets/noanabeshima/TinyStoriesV2</a></p>
            <p id="llama">[3]: <a href="https://doi.org/10.48550/arXiv.2302.13971">arXiv.2302.13971</a></p>
        </section>
    </div>

    <script src="index.js"></script>
</body>
</html>